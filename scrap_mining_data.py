""" Automatically download the shapefiles for basalt producing areas for Majbah """
from bs4 import BeautifulSoup
import requests
from tqdm import tqdm
from urllib.parse import urljoin
import os
##from selenium import webdriver
##from selenium.webdriver.chrome.options import Options


def scan_states_in_html(url):
    """ Loop through the basalt location webpage """

    us_states = ["Arizona", "California", "Colorado", "Connecticut", "Idaho", "Massachusetts", "Maine", "Michigan", "Minnesota", "North_Carolina", "New_Hampshire", "New_Jersey", "New_Mexico", "Nevada", "New_York", "Oklahoma", "Oregon", "South_Dakota", "Tennessee", "Texas", "Utah", "Virginia", "Washington", "Wisconsin", "Wyoming"]

    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.HTTPError as e:
        return f"HTTP Error: {e}"
    except Exception as e:
        return f"Error: {e}"

    soup = BeautifulSoup(response.content, 'html.parser')

    container = soup.body.find('div', class_='container')

    if container:
        for state in us_states:
            state_div = container.find('div', id=state)
            if state_div:
                print(f"Found div for state: {state}")

                links = state_div.find_all('a', href=True)
                for link in tqdm(links):
                    data_link = urljoin("https://mrdata.usgs.gov/geology/state/", link["href"])
                    scan_data_page(data_link)
            else:
                print(f"Div for state '{state}' not found")
    else:
        return "Div with class 'container' not found in the HTML body."

    return "Scanning complete."


def scan_data_page(url):
    """ Scan through the state-specific data page to find the download page """

    ## do not need to use selenium to find buttons because buttons
    ## are generated by js
    ## start chrome in headless (not visible) mode
    ##options = Options()
    ##options.headless = True
    ##driver = webdriver.Chrome(executable_path = "C:/Users/username/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe", options=options)
    ##driver.get(url)
    ##html = driver.page_source
    ##driver.quit()

    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"URL not found or an error occurred: {e}")
        return None

    ##soup = BeautifulSoup(html, 'html.parser')
    soup = BeautifulSoup(response.content, 'html.parser')

    btn_group = soup.body.find('div', class_='btn-group')

    if btn_group:
        buttons = btn_group.find_all('div', class_='btn')

        if buttons:
            last_button = buttons[-1]

            a_tag = last_button.find('a', href=True)
            if a_tag:
                data_link2 = urljoin("https://mrdata.usgs.gov/", a_tag['href'])
                scan_download_page(data_link2)
            else:
                print("No <a> tag with 'href' found in the last button.")
        else:
            print("No buttons found in the 'btn-group' class.")
    else:
        print("Div with class 'btn-group' not found in the HTML body.")

    return


def scan_download_page(url):
    """ Scan through the state-specific data download page to find the download link """
    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"URL not found or an error occurred: {e}")
        return

    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the table in the HTML
    try:
        table = soup.body.find('table')
    except:
        table = False
    if not table:
        print("No table found in the HTML body.")
        return

    # Find the <a> tag within the table body
    a_tag = table.find('a', href=True)
    if not a_tag:
        print("No <a> tag with 'href' found in the table.")
        return

    href = a_tag['href']
    if href.endswith('.zip'):
        # Form the complete URL
        complete_url = urljoin("https://mrdata.usgs.gov/geology/state/", href)
        # Download the zip file
        try:
            zip_content = requests.get(complete_url)
            zip_content.raise_for_status()
            with open(os.path.join(".", "mining_data", href.split('/')[-1]), 'wb') as file:
                file.write(zip_content.content)
            print(f"Zip file downloaded: {href.split('/')[-1]}")
        except requests.RequestException as e:
            print(f"Error downloading zip file: {e}")
    else:
        print(f"Link does not end with '.zip': {href}")


if __name__ == "__main__":
    root_url = "https://mrdata.usgs.gov/geology/state/sgmc-lith.php?text=basalt"
    scan_states_in_html(root_url)